---
title: "Avocados Pricing Engine: Capstone Project"
subtitle: "Data Science Professional Certificate Program (HarvardX - EdX Platform)"
author: "Luis Sousa"
date: "December 2021"
output:
  html_document:
    dev: png
    toc: yes
    fig_width: 10 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. INTRODUCTION

This report is part of the last assignment of the Professional Data Science Certificate program, run by HarvardX in the edX platform. In this final assignment, the students are given the chance to choose freely a topic to apply some of the techniques studied in previous courses of the professional certificate series. To fulfill the requirements of the assignment, a topic with plentiful of data is required.

The **avocado dataset** from the Kaggle platform was chosen for this project, because it is a topic that is simple enough for the majority of people to understand it, but it also allows for the develop advanced data visualization techniques and also advanced machine learning models. 

This report analysis in detail the avocado dataset in four steps:

1. **Data preparation**: Download and cleaning the data 
2. **Data analysis**: Basic data exploration and advanced data visualization
2. **Model development**: Multiple distinct approaches for model engineering
3. **Model selection**: Based on model performance KPIs ($RMSE$, $R^2$)
4. **Conclusion**: Final discussion/comments, including future work

Please note that the complete project comprises of a PDF file, containing the report, and the respective R and RMD files. All omitted code in this report can be found in detail in the R and RMD files.

## 1.1 Goals & Challenges

This capstone project has two main goals, described below:

* Apply to the **avocado** dataset the data science concepts acquired throughout the Professional Certificate Program (HarvardX) in a sinful and innovative way
* Train and validate a model that predicts avocado prices, using different regression approaches

Two KPIs will be used to assess model performance, the $RMSE$ and the $R^2$ (or adjusted $R^2$ for multi regression). The goal of this project is not to set a target for the $RMSE$ or $R^2$, but to analyse how different predictors work on the same basic but yet challenging task of predicting the prices of avocados. As a reminder, the lower $RMSE$ value is, the better the model is. R-squared explains to what extent the variance of one variable explains the variance of the second variable. For instance, a R2 of a model of 0.50, explains approximately half of the observed variation of the model's inputs. Typically, a higher $R^2$ is better and in many finance applications, a $R^2$ close to .7 is deemed very good.

## 1.2 The Avocado Dataset

The avocado dataset was obtained in the [Kaggle](https://www.kaggle.com/) data base. The [avocado dataset](https://www.kaggle.com/neuromusic/avocado-prices) consists of data collected from the Hass Avocado Board website in May of 2018.

The data represents weekly 2015-2018 retail scan data for US National retail volume (in units) and price. The data aggregates several sales channels e.g. grocery, mass, club, drug, dollar and military. The Average Price (of avocados) in the table reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags.

Section 2.3, depicts in more detail the most relevant columns of the avocado dataset for this report.

# 2. DATA ANALYSIS

In this chapter, all steps related to data preparation and analysis will be performed. The first step is to download the data. Then the data will be analysed and explored to derive hypothesis on which models could be used for the rating engine. Additionally, the advanced visualization of the avocado dataset is going to unveil the first report insights.

```{r echo=F, results='hide', eval=T, error=F, warning=F, message=F}
# Note: this process could take a couple of minutes
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org", dependencies = T)
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org", dependencies = T)
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org", dependencies = T)
if(!require(stringi)) install.packages("stringi", repos = "http://cran.us.r-project.org", dependencies = T)
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org", dependencies = T)
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org", dependencies = T)
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org", dependencies = T)
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org", dependencies = T)
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org", dependencies = T)
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org", dependencies = T)
if(!require(corrgram)) install.packages("corrgram", repos = "http://cran.us.r-project.org", dependencies = T)
if(!require(elasticnet)) install.packages("elasticnet", repos = "http://cran.us.r-project.org", dependencies = T)
if(!require(glmnet)) install.packages("glmnet", repos = "http://cran.us.r-project.org", dependencies = T)


library(tidyverse)
library(caret)
library(data.table)
library(stringi)
library(dplyr)
library(knitr)
library(ggplot2)
library(gridExtra)
library(lubridate)
library(corrgram)
library(elasticnet)
library(glmnet)
```

## 2.1 Downloading Data

As explained previously, data for this project was downloaded in the Kaggle website - link to the [avocado dataset](https://www.kaggle.com/neuromusic/avocado-prices). The dataset was replicated on [GitHub](https://github.com/luisfdsousa/EdX_Capstone_Avocado), to allow all the project files to be saved in the same hosting server.

The first section of the R script downloads the avocado dataset in *.cvs* format from GitHub and loads it into a dataframe of raw data. Data will only be downloaded if not present in the current R working directory, to save the script´s running time. The R code is depicted below:

```{r echo=T, results='hide', eval=T, error=F, warning=F, message=F}
# Original Kaggle link with the Avocado dataset
Kaggle_data_link <- 
    "https://www.kaggle.com/neuromusic/avocado-prices/download/"

# Mirrow Avocado dataset on gitHub
gitHub_data_link <- 
  "https://github.com/luisfdsousa/EdX_Capstone_Avocado/raw/main/avocado.csv"

# Download link
data_weblink <- gitHub_data_link

# File name containing the desired dataset
data_file_name <- "avocado.csv"

# Create an empty avocados data frame
avocados <- NULL

#Check if data exists within the working directory
if (file.exists(data_file_name) == F) {
  
  #Info message
  cat("Avocado data not found.",
      "Next step: Download data from Kaggle")
  
  # Create temporary file to store the data
  temp_file <- tempfile()
  
  # Download the data from the web
  download.file(data_weblink, temp_file)
  
  # Open and load .cvs data 
  avocados <- read.csv(temp_file,
                       header = T)
  
  # Free memory, remove temp file
  rm(temp_file)
  
} else {
  
  # Open and load .cvs data 
  avocados <- read.csv(data_file_name,
                       header = T)
}
```

The R packages required for this project were loaded before downloading the data. This step is omitted in this report, but can be analysed in detail in the R script.

## 2.2 Data Transformation & Cleaning

The raw data stored in the **avocados** data frame needs to be transformed to be fit for the next phase of data exploration and visualization. The list of data transformation performed in the **avocados** data frame is as follows: 

* **Filter data**: release from memory the following columns X (data row ID), Total.Bags, Small.Bags, Large.Bags, XLarge.Bags. The number of bags sold is going to be ignored as it is highly correlated with the total volume sold, as it is going to be observed later.
* **Change column names**: Make the **avocados** data set more readable by changing the column names e.g. X4225 is the column of total number of large avocados sold (T4225L).
* **Categorical variables**: Transform strings/characters into categorical variables via factorisation.
* **Dates**: convert dates into POSIX format and create new columns for month and year.
* **Non-Haas avocados**: Create a new column with the total number of non-Haas avocados sold
* **Invalid data points**: Remove all rows containing invalid data points, as these can create substantial challenges during the more advanced sections e.g. advanced data visualisation and model engineering.

Below are the detail steps in R for the initial data transformation phase.

```{r echo=T, results='hide', eval=T, error=F, warning=F, message=F}
# Drop the first column (gibberish)
# drop also the bags column (description not clear)
avocados <- 
  avocados %>%            # pipe the avocados data set
  subset(select = -c(X,
                     Total.Bags,
                     Small.Bags,
                     Large.Bags,
                     XLarge.Bags))

# Rename columns to simplify reading
names(avocados)[names(avocados) == "Total.Volume"] <- "TotalVolume"
names(avocados)[names(avocados) == "X4046"] <- "T4046S"
names(avocados)[names(avocados) == "X4225"] <- "T4225L"
names(avocados)[names(avocados) == "X4770"] <- "T4770XL"
names(avocados)[names(avocados) == "type"] <- "Type"
names(avocados)[names(avocados) == "year"] <- "Year"
names(avocados)[names(avocados) == "region"] <- "Region"

#avocados$Year = as.factor(avocados$Year)
avocados$Type = as.factor(avocados$Type)
avocados$Region = as.factor(avocados$Region)
avocados$Year = as.factor(avocados$Year)
#avocados$Date = as.Date(avocados$Date)
avocados$Date = ymd(avocados$Date)
avocados <- add_column(avocados, Month = factor(months(avocados$Date), levels = month.name))
avocados <- add_column(avocados, Day = factor(format(avocados$Date, format = "%d")))
avocados <- avocados %>% mutate(NonHaasVolume = TotalVolume - T4046S - T4225L - T4770XL)

# Remove NA entries
avocados <- avocados %>% drop_na()
```

Next, the seed is going to be set to achieve repeatable results and the **avocados** data set will be 90/10 split. This means 90% of the avocados data frame is going to be used for the training dataset (**train_data**) and 10% to the testing dataset (**test_data**). The 90/10 split ratio is in this project necessary as the whole dataset is not deemed very large.  The creation of the testing and training datasets are done as follows:

```{r echo=T, results='hide', eval=T, error=F, warning=F, message=F}
# Initiate seed for repeated results
# if using R 3.5 or earlier, use `set.seed(1)`
set.seed(1, sample.kind = "Rounding")

# 90/10 % ratio for test/validation datasets
training_samples <- 
  avocados$AveragePrice %>%
  createDataPartition(p = 0.9, 
                      list = FALSE)

# Create respective train and test datasets 
train_data  <- avocados[training_samples, ]
test_data <- avocados[-training_samples, ]
```

Additional steps to transform the data for some of the models in this report will be required and covered in Chapter 3, as these are specific to the needs of the regression functions in R.

## 2.3 Basic Data Exploration: R Basic Functions

In sections 2.1 and 2.2, the avocado dataset was downloaded and prepared into the training (**train_data**) and testing (**test_data**) datasets. In a typical data science project, the data analysis phase comes before the data preparation phase. In this report the phases are showed in reversed order to showcase the exact same cleaned data version that is used in the model engineering phase.  

This section is fundamental to build the general knowledge about the **avocados** dataset. Bellow, the basic characteristics of the cleaned dataset is showed. These next basic exploration steps are fundamental to get a glimpse on the data structure and to obtain insights about potential model to use for the next chapter.

```{r echo=F, results=T, eval=T, error=F, warning=F, message=F}
# Summary of the data
data_basics_metrics <- c(
  nrow(avocados),
  ncol(avocados),
  anyNA(avocados),
  n_distinct(avocados$Type),
  n_distinct(avocados$Region),
  min(year(avocados$Date)),
  max(year(avocados$Date)))

data_basics_description <- c(
  "Number of Rows",
  "Number of Columns",
  "Number of Invalid Data Points",
  "Distinct Types of Avocados",
  "Distinct Produce Regions",
  "Data Start Date",
  "Data End Date"
)

# Build data frame with columns + description
data_basics <- data.frame(data_basics_description,
                          data_basics_metrics)

# Name the columns of the table
names(data_basics)<-c("Metric","Result")

# Draw the table
data_basics %>%
  kbl(caption = "Basic Data Exploration: Avocado Dataset") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

There are **18.000+ data rows** and **12 columns**, which is far from being considered a large dataset. This may significantly influence the $RMSE$ and $R^2$ of the models. Moreover, the data only spans over a limited amount of time (2015-2018), which may be enough to predict avocado prices into 2019 but not enough to go beyond it, especially if the dataset depicts a recession or a growth phase of the market. There are **54 regions** where avocados are produced in this dataset – 54 regions for 18.000 datapoints means that there are only a few data points region (300 datapoints per region on average). It is confirmed that the dataset has zero invalid data entries, as during the preparation phase all invalid entries were removed. There are two types of avocados: the *organic* and *conventional*. 

The next table depicts some of the project´s most important columns in the **avocados** dataset.

```{r echo=F, results=T, eval=T, error=F, warning=F, message=F}
########################################
# Build column description Table - START
# Save column names
avocados_col <- colnames(avocados)

# Add column meaning
avocados_col_meaning <- c(
  "The date of the observation",
  "The average price of a single avocado",
  "Total number of avocados sold",
  "Total number of avocados: Small/medium Haas",
  "Total number of avocados: Large Haas",
  "Total number of avocados: Extra large Haas",
  "Type of avocado",
  "Produce year",
  "Produce region"
)

# Build data frame with columns + description
results <- data.frame(avocados_col[1:(length(avocados_col) - 3)], 
                      avocados_col_meaning)

# Name the columns of the table
names(results) <- c("Columns", "Description")

# Draw the table
results %>%
  kbl(caption = "Avocado dataset: Columns description") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
# Build column description Table - END
```

Last but not least, a summary of the **avocados** dataset is showed for all the columns. Please remember that the **avocados**, **test_data** and **train_data** data frames share the same data structure.

```{r echo=T, results=T, eval=T, error=F, warning=F, message=F}
# Print data summary
summary(avocados)
```

## 2.4 Advanced Data Exploration: Data Visualization

Section 2.3 was focused on quick data exploration facts and remarks. This section focusses on advanced data visualization. Since the main goal is to predict the avocado's price, most of the plots will be based around the *AveragePrice* column and how it relates to data contained in other columns.

### 2.4.1 Plot 1: Histogram of average prices by avocado type

The first plot shows avocados average price distribution and density for the **avocados** dataset. The mean values are depicted in as the dashed vertical lines. The plots are split by avocado type (organic vs. conventional).

```{r echo=F, results=T, eval=T, error=F, warning=F, message=F}
# Plot #1: Histograms of Average Avocado Prices
par(mar=c(1,1,1,1))

# Calculate mean average  prices per type of avocado
mean_avg_prices <- avocados %>%
  group_by(Type) %>%
  summarise(avg_price = mean(AveragePrice))

# Average Prices histogram
plot1_hist1 <- avocados %>% 
  ggplot(aes(x = AveragePrice, color = Type)) + 
  geom_histogram(binwidth = 0.05, position = "dodge", fill = "white", alpha = 0.5) + 
  facet_grid(Type ~ .) +
  geom_vline(data = mean_avg_prices, aes(xintercept = avg_price, color = Type), linetype = "dashed") + 
  labs(title = "Average Avocado Price", x = "Average Price per Avocado", y = "Count") + 
  theme_classic() + 
  theme(legend.position = "bottom")

# Change line colors by groups
plot1_hist2 <- avocados %>% 
  ggplot(aes(x = AveragePrice, color = Type, fill = Type)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.05, position = "dodge", alpha = 0.2) +
  facet_grid(Type ~ .) +
  geom_density(alpha = 0.3) +
  geom_vline(data = mean_avg_prices, aes(xintercept = avg_price, color = Type), linetype = "dashed") +
  labs(title = "Average Avocado Price (Density)", x = "Average Price per Avocado", y = "Density") +
  theme_classic() + 
  theme(legend.position = "bottom")

# Display plots
grid.arrange(plot1_hist1, plot1_hist2, ncol = 2) # Apply grid.arrange function
```

The key takeaways of Plot 1 are as follows:

* The histograms show that the average prices follow approximately a normal distribution centered around the mean prices for each avocado type. 
* It is clear that organic avocados are on average more expensive than conventional avocados. 
* The type of avocado is a fundamental indicator of the average price and must be taken into consideration during the model engineering phase. 
* **Average avocado price**: *$1.16* (*conventional*), *$1.65* (*organic*)


### 2.4.2 Plot 2: Boxplot of average prices by volume and type

The avocado prices follow a normal distribution, but little is known about the range of average prices. Next, the average avocados are plotted by type and produce year. The goal is to evaluate price trends over the years.

```{r echo=F, results=T, eval=T, error=F, warning=F, message=F}
# Plot #2: Histograms of Average Avocado Prices
plot2_boxplot_price <- avocados %>%
  ggplot(aes(Type, AveragePrice, color = Year)) +
  geom_boxplot() +
  labs(title = "Average Avocado Price", x = "Type of Avocado", y = "Average Price per Avocado") +
  theme_classic() + 
  theme(legend.position = "bottom")

plot2_boxplot_volume <- avocados %>% 
  ggplot(aes(Type, TotalVolume, color = Year)) +
  geom_boxplot() +
  scale_y_continuous(trans='log2') +
  labs(title = "Total Volume Sold", x = "Type of Avocado", y = "Total Volume Sold (in units)") +
  theme_classic() + 
  theme(legend.position = "bottom")

# Display plots
grid.arrange(plot2_boxplot_price, plot2_boxplot_volume, ncol = 2) # Apply grid.arrange function
```

The key takeaways of Plot 2 are as follows:

* The total volume sold for avocados sold have seen an upwards trend over the observed years. The total volume sold of organic avocados have been growing at a steeper rate than that of conventional ones.
* There is significant lower volume of organic avocados available in the market when compared to the conventional ones. This explains partially the difference in average prices of both types of avocados.
* The year of the produce influences greatly the average prices – 2016 and 2018 have seen a drop in the average price. However, it is not straight forward that this represents a typical business cycle: every second year the prices either increase or decrease.
* Outliers are more frequent in the total volume sold of conventional avocados and in the organic average prices. These values can lead to less effective predictive models and must be taken into account.

### 2.4.3 Plot 3: Average prices by volume sold, type, and date

The next plot has two goals, the first one is to understand the business cyclicality over the months and years. The second goal analyses, as a first model hypothesis, whether the total volume sold can be a good proxy for the average avocado prices.

```{r fig.height = 8, echo=F, results=T, eval=T, error=F, warning=F, message=F}
# Plot #3 Total volume sold by Type & Year
plot3_scatterplot_volPrice <- avocados %>%
  ggplot(aes(TotalVolume, AveragePrice, colour = Type)) +
  geom_point(alpha = 0.05) + 
  geom_smooth(color = "black", method = "lm", linetype = "dashed") +
  scale_x_continuous(trans = 'log2') +
  facet_grid(Type ~ .) +
  labs(title = "Average Avocado Price", x = "Total Volume Sold", y = "Average Price per Avocado") +
  theme_classic()

plot3_lineplot_TotalMonthlyRevenue <- avocados %>%
  group_by(Year, Type, Month) %>%
  select(AveragePrice, Year, Month, Type, TotalVolume) %>%
  mutate(MonthlyRevenue = AveragePrice*TotalVolume) %>%
  summarise(TotalMonthlyRevenue = sum(MonthlyRevenue) / 1000000) %>%
  ggplot(aes(x = Month, y = TotalMonthlyRevenue, colour = Year, group = Year)) +
  geom_line() + 
  facet_grid(. ~Type) + 
  scale_y_continuous(trans = 'log2') +
  labs(x = "Month", y = "Monthly Revenue (in mn $)", title = "Average Monthly Avocado Prices") +
  theme_classic() + 
  theme(axis.text.x = element_text(angle = 65, hjust = 1))

# Display plots
grid.arrange(plot3_scatterplot_volPrice, plot3_lineplot_TotalMonthlyRevenue, nrow = 2)
```

The key takeaways of Plot 3 are as follows:

* The function that estimates the average price with the volume sold captures lightly the overall slope trend of prices, but is far from being a good model. Prices seem to get lower with the total volume in an aggregated basis.
* Data for average prices for 2018 is only available up to March. The implications of the lack of data for the remaining months of 2018 is that the models will fail to yet observe a full 4th business cycle. 
* Prices towards the end of each year seem to plummet, while they trend to peak in Summer and Fall.
* The month of the year influences significantly the average price as it captures the typical avocado business cycle in the US
* It seems to be a good time to invest in the organic avocado produce, as the monthly revenue YoY has been increasing every year.
* 2017 was the best business year overall. It is not possible to explain why 2017 was the best year, but it could be due to hidden market factors that are not shown in the data.

### 2.4.4 Plot 4: Total Rev sold by Size, Year & Type

It is clear now that the volume, type and data influence average prices, but little is known about how the different sizes of avocados influence the prices. The next plot attempts at answering this question.

```{r echo=F, results=T, eval=T, error=F, warning=F, message=F}
# Plot #4: Total Rev sold by Size, Year & Type
plot4 <- avocados %>%
  gather(AvocadoSize, Volume, c(T4046S:T4770XL,NonHaasVolume)) %>%
  mutate(Revenue = AveragePrice * Volume) %>% 
  group_by(Year, Month, Type, AvocadoSize) %>%
  summarise(TotalRevenue = sum(Revenue)/1000000) %>%
  ggplot(aes(x = Month, y = TotalRevenue, colour = Year, group = Year)) +
  facet_grid(vars(Type),vars(AvocadoSize), scales="free") +
  geom_line() +
  scale_y_continuous(trans = 'log2') +
  labs(y = "Monthly Revenue (in mn $)", x ="Month", title = "Average Monthly Avocado Prices by Size", colour = 'Year') +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 55, hjust = 1),
        panel.grid.minor = element_line(size = 0.4, linetype = 1),
        panel.grid.major = element_line(size = 0.75, linetype = 3))

# Display plot
plot4
```

The key takeaways of Plot 4 are as follows:

* The monthly revenue for non-Haas avocados have been growing YoY, for both types (organic and conventional).
* The monthly revenue for organic avocados appears to be constant and with little growth for and large avocados.
* Large avocados have the most unstable average prices of all sizes and also these are the avocados that are produced and sold the least.

### 2.4.5 Plot 5: Average Price over time by type

One more plot is required to fully visualize the business cycle of the avocados over the period between 2015 and 2018. This next plot will be the benchmark by which the models in Chapter 3 will be compared.

```{r echo=F, results=T, eval=T, error=F, warning=F, message=F}
# Plot #5: Average Price per date: Conventional/organic Avocados
plot5 <- avocados %>%
  ggplot(aes(x = Date, y = AveragePrice, color = Type)) +
  geom_line(alpha = 0.8) +
  theme_classic() +
  scale_x_date(date_breaks = "6 month", date_labels =  "%b %Y", expand = c(0,0)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

# Display plot
plot5
```

The key takeaways of Plot 5 are as follows:

* Prices vary with high amplitude over the observed period. The reason for this behaviour is the number of outlier data points that the initial dataset contains.
* Prices rise on average during summer, but there is no apparent reason why prices fall so steeply in the beginning of 2018. 
* The price range for organic avocados is clearly larger that that of conventional avocados. 

### 2.4.6 Plot 6: Average price ranges for each region

The last dimension to be analysed in this section is the price per produce region. The next plot highlights how competitive regions are and how they compare with one another.

```{r fig.height = 10, echo=F, results=T, eval=T, error=F, warning=F, message=F}
# Plot #6: Total Rev sold by Size, Year & Type
plot6 <-  avocados %>% 
  ggplot(aes(x = Region, y = AveragePrice, colour = Type)) + #, group = Year
  geom_boxplot() + 
  facet_grid(.~Year, scales = "free") +
  scale_y_continuous(breaks = c(seq(round(min(avocados$AveragePrice),1),
                                    round(max(avocados$AveragePrice),1),0.2)), 
                     limits = c(round(min(avocados$AveragePrice),1),
                                round(max(avocados$AveragePrice),1))) +
  labs(x = "Produce Region", y ="Average Avocado Price", 
       title = "Average prices by region & year") +
  coord_flip() + 
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0),
        panel.grid.minor = element_line(size = 0.4, linetype = 1),
        panel.grid.major = element_line(size = 0.75, linetype = 3))

# Display plot
plot6
```

The key takeaways of Plot 6 are as follows:

* There are 54 regions in the dataset and no one is the same in terms of average price behavior.
* The reason why average prices seem to fluctuate so heavily is due to the distinct behaviors of each US state. There are extreme states, where the average avocado prices vary very aggressively over the year e.g. Portland.
* It will be challenging to perform a regression model over 54 regions, however it is expected that not all will have the same statistical relevance.

# 3. METHODS: MODEL ENGINEERING

Several predictive models are built in this chapter to estimate the average price of avocados. Firstly, a benchmark model is built, the average model, then a multi-regression model is designed and finally a decision tree model will be tested. The judges of the model performances are the $RMSE$ and the $R^2$. Both mathematical expressions can be seen below. 

$$
RMSE=\sqrt{\frac{1}{N}\sum_{u,i}^{}\left (\ \hat{y}_{u,i} - y_{u,i} \right )}
$$

$$
R^{2} = 1 - \frac{\sum (y_{i} - \bar{y_{i}})^{2}}{\sum (y_{i} - \bar{y})^{2}}
$$

The main goal of this section is to explore different methods to predict the average price of avocados and with this exercise observe how the $RMSE$ and $R^2$ evolve. 

## 3.1 Model 1: Average Model

The first model is the simplest, but also one of the most important as it will set the benchmark KPI values to look forward to beat. This model is the average model in which it is assume the same average avocado price to all the samples. The average avocado price is computed using the **train_data** set.

```{r echo=T, results=T, eval=T, error=F, warning=F, message=F}
# MODEL 1: Train model
mean_avocado_price <- mean(train_data$AveragePrice)

# Validate model 1
model1.RMSE <- RMSE(test_data$AveragePrice, mean_avocado_price)
cat("Assuming the same average avocado price of: ", 
    round(mean_avocado_price, 2),
    " the RMSE (Model 1: Average) = ",
    model1.RMSE)
```

From now on the **benchmark** for **RMSE is 0.3989559**. The benchmark $R^2$ will be calculated in the next multi-linear model.  Below is the plot that depicts how well assuming the same average price describes the price fluctuations over time.

```{r echo=F, results=T, eval=T, error=F, warning=F, message=F}
# MODEL 1: Plot results
model1.plot1 <- train_data %>%
  ggplot(aes(x = Date, y = AveragePrice)) + 
  geom_point(color = "gray", alpha = 0.1) +
  scale_x_date(date_breaks = "6 month", date_labels =  "%b %Y", expand = c(0,0)) +
  geom_hline(yintercept = mean_avocado_price, 
             linetype = 1, 
             size = 1) +
  labs(title = "MODEL 1: Average Model", x = "Date", y = "Average Avocado Price") + 
  theme_minimal() +
  geom_text(aes(median(Date), 
                round(mean_avocado_price, 2), 
                label = round(mean_avocado_price, 2),
                vjust = - 1))

# show plot
model1.plot1
```

## 3.2 Model 2: Linear Regression

```{r echo=F, results=F, eval=T, error=F, warning=F, message=F}
# Build Model Results Plot Function
build_validation_plot <- function(test_dataset, estimated_dataset, title, RMSE, R_squared) {
  plot <- test_dataset %>% 
    mutate(model4_price_prediction = estimated_dataset) %>%
    ggplot() + 
    geom_point(aes(x = Date, y = AveragePrice, color = Type)) + 
    geom_line(aes(x = Date, y = model4_price_prediction)) +
    scale_x_date(date_breaks = "6 month", date_labels =  "%b %Y", expand = c(0,0)) +
    labs(title = title, x = "Date", y = "Average Avocado Price") + 
    theme_minimal() + 
    geom_text(aes(median(Date), 
                  max(AveragePrice), 
                  label = paste("RMSE = ", round(RMSE, 4), "R_squared = ",round(R_squared, 4))),
              color = "black") +
    theme(axis.text.x = element_text(angle = 0, vjust = 0),
          panel.grid.minor = element_line(size = 0.4, linetype = 1),
          panel.grid.major = element_line(size = 0.75, linetype = 3),
          legend.position = "bottom")
  return(plot)
}
```
There are a few useful steps that should be done before starting developing regression models. The first step is to observe how the different variables correlate among one another on the **avocados** dataset. This plot is shown below.

```{r echo=F, results=T, eval=T, error=F, warning=F, message=F}
# Correlation matrix: Understand variables correlation
corrgram(avocados, order = TRUE, upper.panel = panel.cor, 
         main = "Correlation matrix: Avocados Dataset")
```

The **corregram** function plots how all columns/variables of the **avocados** dataset are correlated. Here, it is possible to observe that total volume and individual small, large, XL and non-Haas volumes are highly correlated. However, there is no information about the categorical variables. These will be evaluated later during the multi-linear regression model.

```{r echo=T, results=T, eval=T, error=F, warning=F, message=F}
# Prepare data
new_train_data <- train_data %>% select(-c(Date, T4046S, T4225L, T4770XL, Day, NonHaasVolume))
new_train_data <- new_train_data %>% filter(!Region %in% c("RaleighGreensboro", "Chicago", "Boston", "BaltimoreWashington"))
new_train_data.all <- model.matrix(~., new_train_data)[,-1]
new_train_data.x <- model.matrix(AveragePrice~., new_train_data)[,-1]
new_train_data.y <- new_train_data$AveragePrice
new_test_data <- test_data %>% select(-c(T4046S, T4225L, T4770XL, Day, NonHaasVolume))
new_test_data <- new_test_data %>% filter(!Region %in% c("RaleighGreensboro", "Chicago", "Boston", "BaltimoreWashington"))
new_test_data.all <- new_test_data
new_test_data <- new_test_data.all %>% select(-c(Date))
new_test_data.xy <- model.matrix(~., new_test_data)[,-1]
new_test_data.x <- model.matrix(AveragePrice~., new_test_data)[,-1]
new_test_data.y <- new_test_data$AveragePrice
```

Before delving deeper into the regression models, further data preparation is needed. These steps were based on previous small data exploration steps and also based on the evaluation of p-values. The complete list of data transformation steps are as follows:

* Create new training, **new_train_data**, and testing data, **new_test_data**, datasets without the Date, T4046S, T4225L, T4770XL, Day, and NonHaasVolume columns. This step is necessary because some of these variables are highly correlated and won´t add much value to the training of the models
* Remove the regions: *RaleighGreensboro*, *Chicago*, *Boston*, and *BaltimoreWashington* as the p-value in the multi-linear model have shown very high p-values.
* Transform categorical variables such as *Type* and *Region* into a matrix of columns to ease the injection of data for the model training phase.
* Repeat all the above steps for the **new_test_data**

Now that all the data preparation is finalized, it is possible to derive a simple linear model as a function of the *TotalVolume* to determine the $R^2$ benchmark.

```{r echo=T, results=T, eval=T, error=F, warning=F, message=F}
# MODEL 2: Train model
model2 = lm(AveragePrice ~ TotalVolume, data = new_train_data)
model2.summary <- summary(model2)

# Validate model
model2.validation <- model2 %>% predict(new_test_data) %>% as.vector()
model2.RMSE <- RMSE(new_test_data$AveragePrice, model2.validation)
model2.R2 <- model2.summary$r.squared
cat("Model 2 - Linear Model: RMSE =", 
    model2.RMSE,
    "Model 2 - Linear Model: R-squared =", 
    model2.R2)
```

The benchmark for the $R^2$ is **0.03660494**. Notice that the $RMSE$ is not significantly better than the average model $RMSE$:

* Model 1 - Average model: $RMSE = 0.3989559$
* Model 2 - Linear model: $RMSE = 0.3927159, R^2 = 0.03660494$

The plot with the results of model 2 – linear model are depicted below in black.

```{r echo=F, results=T, eval=T, error=F, warning=F, message=F}
# MODEL 2: Plot results
model2.plot1 <- build_validation_plot(new_test_data.all, 
                                      model2.validation, 
                                      "MODEL 2 - Linear Regression",
                                      model2.RMSE,
                                      model2.R2)

# show plot
model2.plot1
```

## 3.3 Model 3: Multi Linear Regression

The model 3 encompasses all variables in the **new_train_data** data frame. The code below represents the refined model after removing all $p-values > .05$. It is expected that this model is significantly better than the previous 2 models. This model is also significantly more complex and harder to compute because of the sheer number of variables.

```{r echo=T, results=T, eval=T, error=F, warning=F, message=F}
# MODEL 3.3: T model: (V, T, M, Y, R)
model3 = lm(AveragePrice ~ TotalVolume + Type + Month + Year + Region, data = new_train_data)
model3.summary <- summary(model3)

# MODEL 3.3: Validate model: (V, T, M, Y, R)
model3.validation <- model3 %>% predict(new_test_data) %>% as.vector()
model3.RMSE <- RMSE(new_test_data$AveragePrice, model3.validation)
model3.R2 <- model3.summary$r.squared
cat("Model 3.3 - Multi Linear Model (V, T, M, Y, R): RMSE =", 
    model3.RMSE,
    "Model 3.3 - Multi Linear Model (V, T, M, Y, R): R-squared =", 
    model3.R2)
```

The first striking point in this model is its ability to follow more closely the business cycles (see plot below). Five variables were considered for this model: the *Total volume sold*, *Type of avocado*, *Produce month*, *Produce year* and *Region*.

* Model 1 - Average model: $RMSE = 0.3989559$
* Model 2 - Linear model: $RMSE = 0.3927159, R^2 = 0.03660494$
* Model 3 – Multi-linear model: $RMSE = 0. 2366201, R^2 = 0. 6420727$

Both the $RMSE$ and $R^2$ values improved significantly and are the new benchmark to beat.

```{r echo=F, results=T, eval=T, error=F, warning=F, message=F}
# MODEL 3.3: Plot results
model3.plot1 <- build_validation_plot(new_test_data.all, 
                                        model3.validation, 
                                        "MODEL 3.3 - Multi Linear Regression (V, T, M, Y, R)",
                                        model3.RMSE,
                                        model3.R2)

# show plot
model3.plot1
```

**Note**: Several combinations of variables used in this model were tried but will not be depicted here because the model results were not as good as this one. 

## 3.3 Model 4: Lasso Regression

Next, the lasso regression model is built. This regression method shrinks values towards a central point, like the mean. This regression performs regularization, which adds a penalty equal to the absolute value of the magnitude of coefficients. Some data points may become zero and eliminated from the model.

```{r echo=T, results=T, eval=T, error=F, warning=F, message=F}
########################################
# MODEL 4.2: Lasso regression (V, T, M, Y, R)
# Evaluate best lambda using cross-validation
model4.2.cv <- cv.glmnet(new_train_data.x, 
                         new_train_data.y, 
                         alpha = 1) #alpha = 1 for lasso regression

# Fit the final model on the training data w/ min lambda
model4.2 <- glmnet(new_train_data.x, 
                   new_train_data.y, 
                   alpha = 1, #alpha = 1 for lasso regression
                   lambda = model4.2.cv$lambda.min)

# MODEL 4.2: Validate model: (V, T, M, Y, R)
model4.2.validation <- model4.2 %>% predict(new_test_data.x) %>% as.vector()
model4.2.RMSE <- RMSE(new_test_data$AveragePrice, model4.2.validation)
model4.2.R2 <- R2(model4.2.validation, new_test_data$AveragePrice)

cat("Model 4 - Lasso Model: RMSE =", 
    model4.2.RMSE,
    "Model 4 - Lasso Model: R-squared =", 
    model4.2.R2)
```

There is therefore a new benchmark to beat. Several variants of linear regressions were tried: Lasso, Ridge, and Elastic Net Regression. However, the lasso model performed the best among these – For more details, please refer to the R script. The results of the lasso model are plotted below.

```{r echo=F, results=T, eval=T, error=F, warning=F, message=F}
# MODEL 4.2: Plot results
model4.2.plot1 <- build_validation_plot(new_test_data.all, 
                                        model4.2.validation, 
                                        "MODEL 4 - Lasso Regression",
                                        model4.2.RMSE,
                                        model4.2.R2)

# show plot
model4.2.plot1
```

## 3.4 Model 4: Ridge & Lasso Regression

https://www.statisticshowto.com/lasso-regression/
https://en.wikipedia.org/wiki/Ridge_regression




BLA


## 3.5 Model Results Summary


# 4. CONCLUSION & DISCUSSION

l

## 4.1 Future considerations

https://data-flair.training/blogs/r-nonlinear-regression/

## 4.2 References

* Irizarry, Rafael A., “Introduction to Data Science: Data Analysis and Prediction Algorithms in R”, webpage:https://rafalab.github.io/dsbook/
* LaTeX Equation Builder, webpage:https://latex.codecogs.com/eqneditor/editor.php
* knitr::kable and kableExtra Tutorial, webpage:https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html
* ggplot2: Quick guide, website:http://www.sthda.com/english/wiki/ggplot2-histogram-plot-quick-start-guide-r-software-and-data-visualization
* Correlograms: website: https://www.statmethods.net/advgraphs/correlograms.html